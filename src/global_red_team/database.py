"""
Production-Ready Secure Database Layer
Fixes SQL injection vulnerabilities and adds proper connection management
"""

import sqlite3
import threading
from contextlib import contextmanager
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class DatabaseError(Exception):
    """Base exception for database operations"""
    pass


class ConnectionPool:
    """Thread-safe SQLite connection pool"""

    def __init__(self, db_path: str, pool_size: int = 5):
        self.db_path = db_path
        self.pool_size = pool_size
        self._local = threading.local()
        self._lock = threading.Lock()

    def _create_connection(self) -> sqlite3.Connection:
        """Create a new database connection"""
        conn = sqlite3.connect(self.db_path, check_same_thread=False)
        conn.row_factory = sqlite3.Row
        conn.execute("PRAGMA foreign_keys = ON")
        conn.execute("PRAGMA journal_mode = WAL")  # Better concurrency
        return conn

    @contextmanager
    def get_connection(self):
        """Get a connection from the pool"""
        if not hasattr(self._local, 'connection'):
            self._local.connection = self._create_connection()

        conn = self._local.connection
        try:
            yield conn
            conn.commit()
        except Exception as e:
            conn.rollback()
            logger.error(f"Database transaction failed: {e}", exc_info=True)
            raise DatabaseError(f"Transaction failed: {str(e)}") from e


class SecureDatabase:
    """Secure database interface with parameterized queries only"""

    # SQL Templates (all use parameterization)
    CREATE_FINDINGS_TABLE = """
        CREATE TABLE IF NOT EXISTS findings (
            id TEXT PRIMARY KEY,
            finding_hash TEXT NOT NULL UNIQUE,
            category TEXT NOT NULL,
            severity TEXT NOT NULL CHECK(severity IN ('critical', 'high', 'medium', 'low', 'info')),
            title TEXT NOT NULL,
            description TEXT,
            affected_component TEXT NOT NULL,
            evidence TEXT,
            remediation TEXT,
            status TEXT NOT NULL DEFAULT 'new' CHECK(status IN ('new', 'open', 'closed', 'false_positive')),
            first_seen TIMESTAMP NOT NULL,
            last_seen TIMESTAMP NOT NULL,
            is_regression BOOLEAN NOT NULL DEFAULT 0,
            cvss_score REAL,
            cwe_id TEXT,
            cve_id TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """

    CREATE_EVIDENCE_TABLE = """
        CREATE TABLE IF NOT EXISTS evidence (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            finding_id TEXT NOT NULL,
            type TEXT,
            content TEXT,
            artifact_url TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (finding_id) REFERENCES findings(id) ON DELETE CASCADE
        )
    """

    CREATE_INDICES = [
        "CREATE INDEX IF NOT EXISTS idx_findings_hash ON findings(finding_hash)",
        "CREATE INDEX IF NOT EXISTS idx_findings_severity ON findings(severity)",
        "CREATE INDEX IF NOT EXISTS idx_findings_status ON findings(status)",
        "CREATE INDEX IF NOT EXISTS idx_findings_last_seen ON findings(last_seen)",
        "CREATE INDEX IF NOT EXISTS idx_evidence_finding ON evidence(finding_id)"
    ]

    def __init__(self, db_path: str = "findings.db", pool_size: int = 5):
        self.db_path = db_path
        self.pool = ConnectionPool(db_path, pool_size)
        self._init_database()

    def _init_database(self):
        """Initialize database schema"""
        with self.pool.get_connection() as conn:
            cursor = conn.cursor()

            # Create tables
            cursor.execute(self.CREATE_FINDINGS_TABLE)
            cursor.execute(self.CREATE_EVIDENCE_TABLE)

            # Create indices
            for index_sql in self.CREATE_INDICES:
                cursor.execute(index_sql)

            logger.info(f"Database initialized at {self.db_path}")

    def save_finding(
        self,
        finding_id: str,
        finding_hash: str,
        category: str,
        severity: str,
        title: str,
        description: str,
        affected_component: str,
        evidence: str,
        remediation: str,
        cvss_score: float = 0.0,
        cwe_id: Optional[str] = None,
        cve_id: Optional[str] = None
    ) -> bool:
        """
        Save or update a finding using parameterized query

        Returns:
            bool: True if new finding, False if updated existing
        """
        now = datetime.now()

        with self.pool.get_connection() as conn:
            cursor = conn.cursor()

            # Check if finding exists (parameterized)
            cursor.execute(
                "SELECT id, status FROM findings WHERE finding_hash = ?",
                (finding_hash,)
            )
            existing = cursor.fetchone()

            if existing:
                # Update existing finding (all parameterized)
                is_regression = existing['status'] == 'closed'

                cursor.execute("""
                    UPDATE findings
                    SET last_seen = ?,
                        status = 'open',
                        is_regression = ?,
                        updated_at = ?
                    WHERE finding_hash = ?
                """, (now, is_regression, now, finding_hash))

                logger.info(f"Updated existing finding: {title} (regression={is_regression})")
                return False
            else:
                # Insert new finding (all parameterized)
                cursor.execute("""
                    INSERT INTO findings (
                        id, finding_hash, category, severity, title,
                        description, affected_component, evidence,
                        remediation, status, first_seen, last_seen,
                        is_regression, cvss_score, cwe_id, cve_id
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, 'new', ?, ?, 0, ?, ?, ?)
                """, (
                    finding_id, finding_hash, category, severity, title,
                    description, affected_component, evidence,
                    remediation, now, now, cvss_score, cwe_id, cve_id
                ))

                logger.info(f"Saved new finding: {title}")
                return True

    def get_finding_by_hash(self, finding_hash: str) -> Optional[Dict[str, Any]]:
        """Get finding by hash (parameterized)"""
        with self.pool.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT * FROM findings WHERE finding_hash = ?",
                (finding_hash,)
            )
            row = cursor.fetchone()
            return dict(row) if row else None

    def get_findings_by_status(
        self,
        status: str,
        limit: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """Get findings by status (parameterized)"""
        with self.pool.get_connection() as conn:
            cursor = conn.cursor()

            query = "SELECT * FROM findings WHERE status = ? ORDER BY severity, last_seen DESC"
            params: Tuple = (status,)

            if limit:
                query += " LIMIT ?"
                params = (status, limit)

            cursor.execute(query, params)
            return [dict(row) for row in cursor.fetchall()]

    def get_findings_by_severity(
        self,
        severity: str,
        status: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """Get findings by severity (parameterized)"""
        with self.pool.get_connection() as conn:
            cursor = conn.cursor()

            if status:
                cursor.execute(
                    "SELECT * FROM findings WHERE severity = ? AND status = ? ORDER BY last_seen DESC",
                    (severity, status)
                )
            else:
                cursor.execute(
                    "SELECT * FROM findings WHERE severity = ? ORDER BY last_seen DESC",
                    (severity,)
                )

            return [dict(row) for row in cursor.fetchall()]

    def close_old_findings(self, run_start_time: datetime) -> int:
        """Mark findings as closed if not seen in current run (parameterized)"""
        with self.pool.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE findings
                SET status = 'closed', is_regression = 0, updated_at = ?
                WHERE last_seen < ? AND status != 'closed'
            """, (datetime.now(), run_start_time))

            count = cursor.rowcount
            logger.info(f"Closed {count} old findings")
            return count

    def get_summary_statistics(self) -> Dict[str, int]:
        """Get summary statistics (parameterized)"""
        with self.pool.get_connection() as conn:
            cursor = conn.cursor()

            # Use parameterized queries for aggregation
            stats = {}

            # Count by severity and status
            cursor.execute("""
                SELECT severity, status, is_regression, COUNT(*) as count
                FROM findings
                GROUP BY severity, status, is_regression
            """)

            for row in cursor.fetchall():
                key = f"{row['severity']}_{row['status']}"
                if row['is_regression']:
                    key = f"{row['severity']}_regression"
                stats[key] = row['count']

            # Total counts
            cursor.execute("SELECT COUNT(*) as total FROM findings")
            stats['total'] = cursor.fetchone()['total']

            cursor.execute("SELECT COUNT(*) as open FROM findings WHERE status IN ('new', 'open')")
            stats['open'] = cursor.fetchone()['open']

            return stats

    def search_findings(
        self,
        search_term: str,
        field: str = 'title'
    ) -> List[Dict[str, Any]]:
        """
        Search findings (parameterized with LIKE)

        Args:
            search_term: Term to search for
            field: Field to search in (title, description, affected_component)
        """
        # Whitelist allowed fields to prevent SQL injection
        allowed_fields = {'title', 'description', 'affected_component', 'evidence'}
        if field not in allowed_fields:
            raise ValueError(f"Invalid field: {field}. Allowed: {allowed_fields}")

        with self.pool.get_connection() as conn:
            cursor = conn.cursor()

            # Safe to use f-string for field name (validated above)
            # But use parameterized query for search term
            query = f"SELECT * FROM findings WHERE {field} LIKE ? ORDER BY severity, last_seen DESC"
            cursor.execute(query, (f"%{search_term}%",))  # nosec

            return [dict(row) for row in cursor.fetchall()]

    def mark_as_false_positive(self, finding_hash: str, reason: str = "") -> bool:
        """Mark a finding as false positive (parameterized)"""
        with self.pool.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE findings
                SET status = 'false_positive',
                    description = description || ? || ?,
                    updated_at = ?
                WHERE finding_hash = ?
            """, ('\n\n[False Positive Reason]: ', reason, datetime.now(), finding_hash))

            return cursor.rowcount > 0

    def get_trend_data(self, days: int = 30) -> Dict[str, List[Tuple[str, int]]]:
        """Get trend data for dashboards (parameterized)"""
        with self.pool.get_connection() as conn:
            cursor = conn.cursor()

            # Get findings over time
            cursor.execute("""
                SELECT DATE(first_seen) as date, severity, COUNT(*) as count
                FROM findings
                WHERE first_seen >= DATE('now', '-' || ? || ' days')
                GROUP BY DATE(first_seen), severity
                ORDER BY date, severity
            """, (days,))

            trends = {}
            for row in cursor.fetchall():
                severity = row['severity']
                if severity not in trends:
                    trends[severity] = []
                trends[severity].append((row['date'], row['count']))

            return trends

    def backup_database(self, backup_path: str):
        """Create a backup of the database"""
        import shutil
        backup_path = Path(backup_path)
        backup_path.parent.mkdir(parents=True, exist_ok=True)

        with self.pool.get_connection() as conn:
            # Ensure all changes are committed
            conn.commit()

        shutil.copy2(self.db_path, backup_path)
        logger.info(f"Database backed up to {backup_path}")

    def vacuum_database(self):
        """Optimize database (VACUUM)"""
        with self.pool.get_connection() as conn:
            conn.execute("VACUUM")
            logger.info("Database vacuumed")